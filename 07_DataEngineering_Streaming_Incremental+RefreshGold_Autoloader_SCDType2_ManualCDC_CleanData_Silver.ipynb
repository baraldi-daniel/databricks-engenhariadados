{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a6ae56-67e3-4779-8e53-c845b890bfd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![./ImageLab.png](./Images/ImageLab.png \"./ImageLab.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db7e527-9f90-43ed-b873-196601fc6d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Engineering with Lakeflow, Jobs, AutoLoader and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c60b7f-f190-424f-bb5d-eea0ec3c6ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_LOCATION = dbutils.widgets.get(\"param_location\")+\"/checkpoint\"\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3527685d-9b93-4bb2-b3ac-fa2cd83350bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data for Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2911c0c7-8339-4799-a3b8-a238a45916c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading table\n",
    "df_bronze=spark.readStream \\\n",
    "  .table(\"medallion_autoloader.bronze.dim_customer\")\n",
    "\n",
    "display(df_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad30b4e-5b9b-4db2-ad9d-27f8a73b58ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Treating data\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Treating customer_bk\n",
    "df_bronze_treated = df_bronze \\\n",
    "  .withColumn(\"customer_bk\", F.col('customer_bk').cast(\"string\")) \\\n",
    "  .withColumn(\"customer_bk\", F.regexp_replace(F.col('customer_bk'), \"[^0-9]\", \"\"))\n",
    "\n",
    "# Treating customer_name\n",
    "df_bronze_treated = df_bronze_treated \\\n",
    "  .withColumn(\"customer_name\", F.trim(F.col('customer_name')))\n",
    "\n",
    "# Treating birth_date\n",
    "#Alternative 1 with coalesce\n",
    "# formats = [\"dd/MM/yyyy\", \"yyyy-MM-dd\", \"MM/dd/yyyy\", \"dd-MM-yyyy\"]\n",
    "# df_bronze_treated = df_bronze_treated.withColumn(\"birth_date\",F.coalesce(*[F.try_to_date(\"birth_date\", f) for f in formats]))\n",
    "\n",
    "# Alternative 2 using ai function\n",
    "df_bronze_treated = df_bronze_treated.selectExpr(\"* EXCEPT(birth_date)\",\"ai_query('databricks-meta-llama-3-3-70b-instruct',concat('Padronize as datas para o formato 2025-01-01. Apenas me dê o resulado final e nada mais: ', birth_date)) as birth_date\")\n",
    "\n",
    "# Treating segment\n",
    "df_bronze_treated = df_bronze_treated.withColumn(\"segment\", F.trim(F.initcap(F.col('segment'))))\n",
    "\n",
    "# Treating region\n",
    "df_bronze_treated = df_bronze_treated.selectExpr(\"* EXCEPT(region)\",\"ai_query('databricks-meta-llama-3-3-70b-instruct',concat('Padronize as regiões para o formato de sigla do estado do Brasil, como SP, MG, RJ, etc. Apenas me dê o resulado final e nada mais: ', region)) as region\")\n",
    "\n",
    "# Treating effective_ts\n",
    "fmt_iso = \"yyyy-MM-dd'T'HH:mm:ss'Z'\"\n",
    "fmt_space = \"yyyy-MM-dd HH:mm:ss\"\n",
    "\n",
    "df_bronze_treated = df_bronze_treated.withColumn(\n",
    "    \"effective_ts_clean\", \n",
    "    F.coalesce(\n",
    "        F.try_to_timestamp(F.col(\"effective_ts\"), F.lit(fmt_iso)),\n",
    "        F.try_to_timestamp(F.col(\"effective_ts\"), F.lit(fmt_space))\n",
    "    )\n",
    ").drop(\"effective_ts\").withColumnRenamed(\"effective_ts_clean\", \"effective_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf430236-dc03-4f0c-a955-786689a3ff16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "df_bronze_stream = df_bronze_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957a1943-ad30-415a-8cd3-4e1cfc7da2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "TARGET_TABLE = \"medallion_autoloader.silver.dim_customer\"\n",
    "\n",
    "def upsert_batch(df_bronze_batch, batch_id):\n",
    "    w_last = Window.partitionBy(\"customer_bk\").orderBy(F.col(\"effective_ts\").desc())\n",
    "    df_bronze_dedup = (\n",
    "        df_bronze_batch\n",
    "        .select(\"customer_bk\", \"customer_name\", \"birth_date\", \"segment\", \"region\", \"effective_ts\", \"file_path\", \"file_mod_time\") \n",
    "        .withColumn(\"rn\", F.row_number().over(w_last))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    if not spark.catalog.tableExists(TARGET_TABLE):\n",
    "        df_init = (\n",
    "            df_bronze_dedup\n",
    "            .withColumn(\"valid_from\", F.col(\"effective_ts\"))\n",
    "            .withColumn(\"valid_to\",   F.lit(None).cast(\"timestamp\"))\n",
    "            .withColumn(\"is_current\", F.lit(True))\n",
    "            .withColumn(\"version\",    F.lit(1).cast(\"int\"))\n",
    "        )\n",
    "\n",
    "        w_sk_init = Window.orderBy(\"customer_bk\", \"valid_from\", \"customer_name\", \"birth_date\")\n",
    "        df_init = df_init.withColumn(\n",
    "            \"customer_sk\",\n",
    "            F.row_number().over(w_sk_init).cast(\"bigint\")\n",
    "        )\n",
    "\n",
    "        df_init = df_init.select(\n",
    "            \"customer_bk\",\n",
    "            \"customer_name\",\n",
    "            \"birth_date\",\n",
    "            \"segment\",\n",
    "            \"region\",\n",
    "            \"effective_ts\",\n",
    "            \"valid_from\",\n",
    "            \"valid_to\",\n",
    "            \"is_current\",\n",
    "            \"version\",\n",
    "            \"customer_sk\",\n",
    "            \"file_path\",\n",
    "            \"file_mod_time\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            (\n",
    "                df_init\n",
    "                .write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"error\")\n",
    "                .saveAsTable(TARGET_TABLE)\n",
    "            )\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if \"DELTA_TABLE_ALREADY_EXISTS\" in str(e) or \"already exists\" in str(e):\n",
    "                pass\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    delta = DeltaTable.forName(spark, TARGET_TABLE)\n",
    "\n",
    "    (\n",
    "        delta.alias(\"delta\")\n",
    "        .merge(\n",
    "            source=df_bronze_dedup.alias(\"df\"),\n",
    "            condition=\"df.customer_bk = delta.customer_bk\"\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"\"\"\n",
    "                delta.is_current = true AND (\n",
    "                    delta.customer_name <> df.customer_name OR\n",
    "                    delta.birth_date    <> df.birth_date    OR\n",
    "                    delta.segment       <> df.segment       OR\n",
    "                    delta.region        <> df.region\n",
    "                )\n",
    "            \"\"\",\n",
    "            set={\n",
    "                \"is_current\": F.lit(False),\n",
    "                \"valid_to\":   F.col(\"df.effective_ts\"),\n",
    "            }\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    dim_customer = spark.read.table(TARGET_TABLE)\n",
    "    dim_current  = dim_customer.filter(F.col(\"is_current\") == True)\n",
    "\n",
    "    df_join = df_bronze_dedup.alias(\"df\").join(\n",
    "        dim_current.alias(\"delta\"),\n",
    "        on=[F.col(\"df.customer_bk\") == F.col(\"delta.customer_bk\")],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_new = df_join.filter(F.col(\"delta.customer_bk\").isNull()).select(\"df.*\")\n",
    "\n",
    "    df_changed = df_join.filter(\n",
    "        (F.col(\"delta.customer_bk\").isNotNull()) &\n",
    "        (\n",
    "            (F.col(\"delta.customer_name\") != F.col(\"df.customer_name\")) |\n",
    "            (F.col(\"delta.birth_date\")    != F.col(\"df.birth_date\"))    |\n",
    "            (F.col(\"delta.segment\")       != F.col(\"df.segment\"))       |\n",
    "            (F.col(\"delta.region\")        != F.col(\"df.region\"))\n",
    "        )\n",
    "    ).select(\"df.*\")\n",
    "\n",
    "    df_remaining = df_new.unionByName(df_changed)\n",
    "\n",
    "    if df_remaining.limit(1).count() == 0:\n",
    "        return\n",
    "\n",
    "    df_remaining = (\n",
    "        df_remaining\n",
    "        .withColumn(\"valid_from\", F.col(\"effective_ts\"))\n",
    "        .withColumn(\"valid_to\",   F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "    )\n",
    "\n",
    "    max_ver_por_bk = dim_customer.groupBy(\"customer_bk\") \\\n",
    "        .agg(F.max(\"version\").alias(\"max_version\"))\n",
    "\n",
    "    df_remaining = (\n",
    "        df_remaining\n",
    "        .join(max_ver_por_bk, on=\"customer_bk\", how=\"left\")\n",
    "        .withColumn(\"max_version\", F.coalesce(F.col(\"max_version\"), F.lit(0)))\n",
    "        .withColumn(\"version\", (F.col(\"max_version\") + F.lit(1)).cast(\"int\"))\n",
    "        .drop(\"max_version\")\n",
    "    )\n",
    "\n",
    "    max_sk = dim_customer.agg(F.max(\"customer_sk\").alias(\"max_sk\")).collect()[0][\"max_sk\"]\n",
    "    if max_sk is None:\n",
    "        max_sk = 0\n",
    "\n",
    "    w_sk = Window.orderBy(\"valid_from\", \"customer_bk\", \"customer_name\", \"birth_date\")\n",
    "    df_remaining = df_remaining.withColumn(\n",
    "        \"customer_sk\",\n",
    "        (F.row_number().over(w_sk) + F.lit(max_sk)).cast(\"bigint\")\n",
    "    )\n",
    "\n",
    "    delta = DeltaTable.forName(spark, TARGET_TABLE)\n",
    "\n",
    "    (\n",
    "        delta.alias(\"delta\")\n",
    "        .merge(\n",
    "            source=df_remaining.alias(\"df\"),\n",
    "            condition=\"delta.customer_sk = df.customer_sk\"\n",
    "        )\n",
    "        .whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"customer_sk\":   F.col(\"df.customer_sk\"),\n",
    "                \"customer_bk\":   F.col(\"df.customer_bk\"),\n",
    "                \"customer_name\": F.col(\"df.customer_name\"),\n",
    "                \"birth_date\":    F.col(\"df.birth_date\"),\n",
    "                \"segment\":       F.col(\"df.segment\"),\n",
    "                \"region\":        F.col(\"df.region\"),\n",
    "                \"valid_from\":    F.col(\"df.valid_from\"),\n",
    "                \"valid_to\":      F.col(\"df.valid_to\"),\n",
    "                \"is_current\":    F.col(\"df.is_current\"),\n",
    "                \"version\":       F.col(\"df.version\"),\n",
    "                \"file_path\":     F.col(\"df.file_path\"),\n",
    "                \"file_mod_time\": F.col(\"df.file_mod_time\")\n",
    "            }\n",
    "        )\n",
    "        .execute()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03eeda33-52b3-49c1-a233-ab8776a2c06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df_bronze_stream\n",
    "    .writeStream\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_LOCATION+\"/silver/dim_customer\")\n",
    "    .foreachBatch(upsert_batch)\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_DataEngineering_Streaming_Incremental+RefreshGold_Autoloader_SCDType2_ManualCDC_CleanData_Silver",
   "widgets": {
    "param_location": {
     "currentValue": "abfss://container@baraldistorage.dfs.core.windows.net/",
     "nuid": "234e921f-5b8b-49ed-a673-25a653904ca3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "param_location",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "param_location",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
