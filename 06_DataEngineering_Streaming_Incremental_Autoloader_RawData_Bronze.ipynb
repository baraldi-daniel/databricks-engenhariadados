{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf961bd-026e-4d8e-ad98-b333e6f364ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![./ImageLab.png](./Images/ImageLab.png \"./ImageLab.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c9bbca-c57e-4e86-bdb6-f1cdd9ce22fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Engineering with Lakeflow, Jobs, AutoLoader and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b469d68-49ba-40a3-8ad7-59c3bb70a3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalog and schema creation and deletion of tables in case they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b34273-203b-47a7-a59f-ff17846b51b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LOCATION_VAR = dbutils.widgets.get(\"param_location\")\n",
    "SCHEMA_LOCATION = dbutils.widgets.get(\"param_location\")+\"/schema\"\n",
    "CHECKPOINT_LOCATION = dbutils.widgets.get(\"param_location\")+\"/checkpoint\"\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8d1ed5-4687-43b4-b292-bc875349bac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS medallion_autoloader MANAGED LOCATION '{LOCATION_VAR}/catalogautoloader'\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS medallion_autoloader.bronze MANAGED LOCATION '{LOCATION_VAR}/catalogautoloader/bronze'\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS medallion_autoloader.silver MANAGED LOCATION '{LOCATION_VAR}/catalogautoloader/silver'\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS medallion_autoloader.gold MANAGED LOCATION '{LOCATION_VAR}/catalogautoloader/gold'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e07aa03-d2c9-49dd-bc9b-5a552a564019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Table Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b862d16-6db8-4611-b080-e1a86f14d9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since this is an incremental streaming use case, we first need to load the data. For this scenario, we will fetch data from Object Storage (now leveraging checkpointing capabilities provided by AutoLoader and CDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbc6983-d678-42b1-932a-f3574969f2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Customer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0a7ce2-7837-4c5b-ac08-4f9a51b1ee92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading files using AutoLoader\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "csv_schema = StructType([\n",
    "    StructField(\"customer_bk\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"birth_date\", StringType(), True),\n",
    "    StructField(\"segment\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"effective_ts\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_raw_customer=spark.readStream \\\n",
    "  .format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", SCHEMA_LOCATION+\"/customer\") \\\n",
    "  .schema(csv_schema) \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(f\"{LOCATION_VAR}/data_medallion/customer\") \\\n",
    "  .selectExpr(\n",
    "            \"*\",\n",
    "            \"_metadata.file_path as file_path\",\n",
    "            \"_metadata.file_modification_time as file_mod_time\"\n",
    "        )\n",
    "\n",
    "\n",
    "df_raw_customer \\\n",
    "  .writeStream \\\n",
    "  .option(\"checkpointLocation\", CHECKPOINT_LOCATION) \\\n",
    "  .trigger(availableNow=True) \\\n",
    "  .toTable(\"medallion_autoloader.bronze.dim_customer\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"ALTER TABLE medallion_autoloader.bronze.dim_customer\n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = false)\"\"\")\n",
    "\n",
    "display(spark.table(\"medallion_autoloader.bronze.dim_customer\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_DataEngineering_Streaming_Incremental_Autoloader_RawData_Bronze",
   "widgets": {
    "param_location": {
     "currentValue": "abfss://container@baraldistorage.dfs.core.windows.net/",
     "nuid": "b56a54ae-190f-48e6-b4fc-3bc1cf37ab78",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "param_location",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "param_location",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
