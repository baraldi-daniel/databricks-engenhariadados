{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fc5ce3d-68fe-4844-bbd6-5246475e55e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](Images/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24c86cc-941e-4672-984c-ce07bc8a8bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimizations Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ad8b8-492d-46a2-b674-fc931b8c0aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are plenty of optimizations that Databricks does by default or that are required to be enabled. This notebook shows some of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b2fc870-d6dd-414f-b1a1-51d1151a3c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Performance\n",
    "### Modern Optimizations vs. Legacy Methods\n",
    "\n",
    "This notebook benchmarks the performance of Databricks modern features (Liquid Clustering, Photon, Deletion Vectors) against legacy strategies (Partitioning + Z-Order) and unoptimized baselines.\n",
    "\n",
    "**Cluster Config:** 2 Workers (Fixed), Photon Enabled.\n",
    "\n",
    "**Test Scenarios:**\n",
    "1.  **Data Layout:** Standard vs. Partitioning+ZOrder vs. Liquid Clustering.\n",
    "2.  **DML Operations:** Copy-on-Write vs. Deletion Vectors.\n",
    "3.  **Read Speed:** Cold Read vs. Disk Cache (Hot).\n",
    "4.  **ETL Merge:** Standard Shuffle vs. Low Shuffle Merge.\n",
    "5.  **Search:** Full Scan vs. Bloom Filters.\n",
    "6.  **DevOps:** Deep Clone vs. Shallow Clone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cc1d0e3-fe67-4abb-ad41-ee3560f76d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2092958-9d8a-467d-abf9-5ac1f1617f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "target_catalog = \"catalog_databricks\"\n",
    "target_schema  = \"schema\"  # The specific schema you requested\n",
    "\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0].split('@')[0]\n",
    "base_path = f\"dbfs:/tmp/{username}/benchmark_clickstream_source\"\n",
    "\n",
    "print(f\"ðŸš€ Starting Unity Catalog Setup...\")\n",
    "print(f\"ðŸ“ Catalog: {target_catalog}\")\n",
    "print(f\"ðŸ“ Schema:   {target_schema}\")\n",
    "\n",
    "# 1. Schema Setup (Clean & Create)\n",
    "try:\n",
    "    spark.sql(f\"USE CATALOG {target_catalog}\")\n",
    "    # WARNING: DROP CASCADE deletes everything in this schema to ensure a clean test\n",
    "    spark.sql(f\"DROP SCHEMA IF EXISTS {target_schema} CASCADE\")\n",
    "    spark.sql(f\"CREATE SCHEMA {target_schema}\")\n",
    "    spark.sql(f\"USE SCHEMA {target_schema}\")\n",
    "    print(f\"âœ… Schema '{target_schema}' created and selected.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error manipulating schema. Check permissions.\")\n",
    "    raise e\n",
    "\n",
    "# 2. Benchmark Functions\n",
    "results = []\n",
    "\n",
    "def run_benchmark(test_name, method, func):\n",
    "    print(f\"ðŸš€ Running: {test_name} [{method}]...\")\n",
    "    spark.catalog.clearCache() \n",
    "    start_ts = time.time()\n",
    "    func()\n",
    "    end_ts = time.time()\n",
    "    duration = end_ts - start_ts\n",
    "    print(f\"   â±ï¸ Time: {duration:.4f} seconds\\n\")\n",
    "    results.append({\"Test\": test_name, \"Method\": method, \"Duration_Sec\": duration})\n",
    "\n",
    "def display_results(prefix):\n",
    "    section_data = [r for r in results if r['Test'].startswith(prefix)]\n",
    "    if not section_data: return\n",
    "    \n",
    "    df_pd = pd.DataFrame(section_data)\n",
    "    \n",
    "    # Baseline Logic\n",
    "    def get_baseline_duration(test_name):\n",
    "        baseline_row = df_pd[\n",
    "            (df_pd['Test'] == test_name) & \n",
    "            (df_pd['Method'].str.contains(\"Standard\") | df_pd['Method'].str.contains(\"Legacy\") | df_pd['Method'].str.contains(\"Cold\"))\n",
    "        ]\n",
    "        if not baseline_row.empty: return baseline_row.iloc[0]['Duration_Sec']\n",
    "        return df_pd[df_pd['Test'] == test_name]['Duration_Sec'].max()\n",
    "\n",
    "    df_pd['Baseline'] = df_pd['Test'].apply(get_baseline_duration)\n",
    "    df_pd['Speedup'] = df_pd['Baseline'] / df_pd['Duration_Sec']\n",
    "    \n",
    "    # Ranking Logic\n",
    "    def get_rank(method_name):\n",
    "        if \"Standard\" in method_name or \"Legacy\" in method_name: return 1\n",
    "        return 2\n",
    "    \n",
    "    df_pd['Rank'] = df_pd['Method'].apply(get_rank)\n",
    "    df_pd['Speedup (vs Std)'] = df_pd['Speedup'].apply(lambda x: f\"{x:.1f}x\")\n",
    "    df_pd['Duration_Sec'] = df_pd['Duration_Sec'].apply(lambda x: round(x, 4))\n",
    "    \n",
    "    final_df = df_pd.sort_values(by=['Test', 'Rank'])[['Test', 'Method', 'Duration_Sec', 'Speedup (vs Std)']]\n",
    "    \n",
    "    print(f\"--- ðŸ“Š Partial Results ({prefix}) ---\")\n",
    "    display(spark.createDataFrame(final_df))\n",
    "\n",
    "def get_dir_size_mb(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        total_bytes = sum([f.size for f in files if f.name.endswith(\".parquet\") or f.name.endswith(\".csv\")])\n",
    "        return total_bytes / (1024 * 1024)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# --- DATA GENERATION ---\n",
    "num_rows = 15000000 \n",
    "print(f\"Generating {num_rows/1000000}M clickstream events...\")\n",
    "\n",
    "df_source = spark.range(0, num_rows) \\\n",
    "    .withColumn(\"user_id\", (F.col(\"id\") % 500000).cast(IntegerType())) \\\n",
    "    .withColumn(\"session_id\", F.rand() * 1000000) \\\n",
    "    .withColumn(\"url\", F.lit(\"/checkout/confirmation\")) \\\n",
    "    .withColumn(\"event_date\", F.date_add(F.lit(\"2024-01-01\"), (F.col(\"id\") % 30).cast(IntegerType()))) \\\n",
    "    .withColumn(\"event_hour\", (F.col(\"id\") % 24).cast(IntegerType()))\n",
    "\n",
    "# --- STORAGE EFFICIENCY CHECK ---\n",
    "print(\"Calculating Storage Efficiency...\")\n",
    "\n",
    "# 1. RAW Size (Uncompressed, simulating bad CSV/Parquet)\n",
    "raw_path = f\"{base_path}/check_raw_uncompressed\"\n",
    "dbutils.fs.rm(raw_path, True) # Clean if exists\n",
    "df_source.write.format(\"parquet\").option(\"compression\", \"uncompressed\").mode(\"overwrite\").save(raw_path)\n",
    "raw_size_mb = get_dir_size_mb(raw_path)\n",
    "\n",
    "# 2. Optimized Size (Delta Lake with Snappy/ZSTD)\n",
    "# Save a temp table just to check managed size\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_size_check\")\n",
    "df_source.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"tbl_size_check\")\n",
    "\n",
    "# Get real Delta table size from metadata\n",
    "delta_size_bytes = spark.sql(\"DESCRIBE DETAIL tbl_size_check\").select(\"sizeInBytes\").collect()[0][0]\n",
    "opt_size_mb = delta_size_bytes / (1024 * 1024)\n",
    "\n",
    "# Drop check table\n",
    "spark.sql(\"DROP TABLE tbl_size_check\")\n",
    "\n",
    "savings = raw_size_mb / opt_size_mb if opt_size_mb > 0 else 0\n",
    "\n",
    "# Cache to speed up next steps\n",
    "df_source.cache() \n",
    "df_source.count() # Force cache materialization\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"--- Data Volume & Compression Analysis ---\")\n",
    "print(f\"Rows Generated           : {num_rows:,}\")\n",
    "print(f\"Raw Size (Uncompressed)  : {raw_size_mb:.2f} MB\")\n",
    "print(f\"Delta Size (Compressed)  : {opt_size_mb:.2f} MB\")\n",
    "print(f\"Storage Efficiency       : {savings:.1f}x Smaller\")\n",
    "print(f\"Schema Used              : {target_catalog}.{target_schema}\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11e23c72-9184-45d0-a618-be6d5712f7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4867bae3-b518-4e06-b43b-ef141ff19673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_user = 42000\n",
    "\n",
    "# ==========================================\n",
    "# 1. Standard (Partitioned only - The Baseline)\n",
    "# ==========================================\n",
    "def write_standard():\n",
    "    # Writes data partitioned by Date/Hour.\n",
    "    # This is the default behavior for most Data Lakes.\n",
    "    # It creates ~720 directories.\n",
    "    df_source.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "        .partitionBy(\"event_date\", \"event_hour\") \\\n",
    "        .saveAsTable(\"tbl_standard\")\n",
    "\n",
    "run_benchmark(\"1. Ingestion\", \"Standard (Part Only)\", write_standard)\n",
    "\n",
    "def query_standard():\n",
    "    # Problem: Spark must open 720 directories.\n",
    "    # Inside each directory, it has to scan the whole file because there is no Z-Order.\n",
    "    spark.read.table(\"tbl_standard\").filter(f\"user_id = {target_user}\").count()\n",
    "\n",
    "run_benchmark(\"1. Read (Filter)\", \"Standard (Slow Scan)\", query_standard)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Legacy (Partitioned + Z-Order - The \"Tax\")\n",
    "# ==========================================\n",
    "def write_legacy_zorder():\n",
    "    # Step 1: Write Partitioned (Same as Standard)\n",
    "    df_source.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "        .partitionBy(\"event_date\", \"event_hour\") \\\n",
    "        .saveAsTable(\"tbl_legacy_zorder\")\n",
    "    \n",
    "    # Step 2: The \"Hidden Tax\". We runs Z-ORDER to fix the read speed.\n",
    "    # This reads and rewrites the data again!\n",
    "    spark.sql(\"OPTIMIZE tbl_legacy_zorder ZORDER BY (user_id)\")\n",
    "\n",
    "run_benchmark(\"1. Ingestion\", \"Legacy (Part + ZOrder)\", write_legacy_zorder)\n",
    "\n",
    "def query_legacy_zorder():\n",
    "    # Z-Order helps skip data WITHIN the files, but Spark still lists 720 directories.\n",
    "    spark.read.table(\"tbl_legacy_zorder\").filter(f\"user_id = {target_user}\").count()\n",
    "\n",
    "run_benchmark(\"1. Read (Filter)\", \"Legacy (ZOrder Read)\", query_legacy_zorder)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. Liquid Clustering (The Solution)\n",
    "# ==========================================\n",
    "def write_liquid():\n",
    "    # Liquid handles everything in ONE pass.\n",
    "    # No partitions to manage. No separate Z-Order command.\n",
    "    df_source.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"liquidClustering\", \"true\") \\\n",
    "        .clusterBy(\"user_id\") \\\n",
    "        .saveAsTable(\"tbl_liquid\")\n",
    "\n",
    "run_benchmark(\"1. Ingestion\", \"Liquid Clustering\", write_liquid)\n",
    "\n",
    "def query_liquid():\n",
    "    # Liquid goes straight to the file containing the user.\n",
    "    # Zero directory listing overhead.\n",
    "    spark.read.table(\"tbl_liquid\").filter(f\"user_id = {target_user}\").count()\n",
    "\n",
    "run_benchmark(\"1. Read (Filter)\", \"Liquid Clustering\", query_liquid)\n",
    "\n",
    "display_results(\"1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ba9610-96b2-402a-a0fc-8cde9213b788",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"url\":150},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767887484975}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM catalog_databricks.schema.tbl_standard LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "919e0e59-88a7-41e1-b625-21971efb905a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DML Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb348c17-1eb6-4d40-a79e-f30b89d75b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- AUTO-CLEANUP ---\n",
    "# Remove results from previous runs of Section 2 to avoid duplicate rows\n",
    "global results\n",
    "results = [r for r in results if not r['Test'].startswith(\"2.\")]\n",
    "\n",
    "# ==========================================\n",
    "# DIMENSION 2: DML OPERATIONS (Delete/Update)\n",
    "# Strategy: Create a Single Giant File (~1GB).\n",
    "# CoW must rewrite the whole GB. DV touches nothing.\n",
    "# ==========================================\n",
    "\n",
    "print(\"Preparing High-Volume Data for DML Test...\")\n",
    "\n",
    "# 1. READ & BULK UP\n",
    "# We take the previous data and add a 'payload' column to make it physically heavy.\n",
    "# This ensures the file is large enough (~500MB+) to make Copy-on-Write painful.\n",
    "df_heavy = spark.read.table(\"tbl_liquid\") \\\n",
    "    .withColumn(\"payload\", F.expr(\"repeat('DATA_LAKE_SCALE_', 50)\")) \\\n",
    "    .repartition(1) # FORCE 1 SINGLE GIANT FILE\n",
    "\n",
    "# Materialize this \"Giant File\" table\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_dml_source\")\n",
    "df_heavy.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"tbl_dml_source\")\n",
    "\n",
    "print(\"Tables prepared. Creating competitors...\")\n",
    "\n",
    "# 2. Setup Competitors\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_cow\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_dv\")\n",
    "\n",
    "# A: Copy-on-Write Table (Legacy / Standard)\n",
    "spark.sql(\"CREATE TABLE tbl_cow CLONE tbl_dml_source\")\n",
    "spark.sql(\"ALTER TABLE tbl_cow SET TBLPROPERTIES ('delta.enableDeletionVectors' = false)\")\n",
    "\n",
    "# B: Deletion Vectors Table (Modern)\n",
    "spark.sql(\"CREATE TABLE tbl_dv CLONE tbl_dml_source\")\n",
    "spark.sql(\"ALTER TABLE tbl_dv SET TBLPROPERTIES ('delta.enableDeletionVectors' = true)\")\n",
    "\n",
    "\n",
    "# --- TEST A: COPY-ON-WRITE (Standard) ---\n",
    "def delete_cow():\n",
    "    # Scenario: Delete sparse records (approx 0.1% of data)\n",
    "    # The Penalty: Spark must Read 1GB -> Filter -> Rewrite 0.99GB.\n",
    "    spark.sql(\"DELETE FROM tbl_cow WHERE id % 1000 = 999\")\n",
    "\n",
    "run_benchmark(\"2. Delete\", \"Copy-on-Write (Standard)\", delete_cow)\n",
    "\n",
    "\n",
    "# --- TEST B: DELETION VECTORS (Modern) ---\n",
    "def delete_dv():\n",
    "    # The Gain: Spark marks rows as deleted in a tiny metadata file.\n",
    "    # The 1GB file is NOT rewritten.\n",
    "    spark.sql(\"DELETE FROM tbl_dv WHERE id % 1000 = 999\")\n",
    "\n",
    "run_benchmark(\"2. Delete\", \"Deletion Vectors (Modern)\", delete_dv)\n",
    "\n",
    "\n",
    "# --- SHOW COMPARISON ---\n",
    "display_results(\"2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c45486ef-3d66-44d1-a8ee-f1739fbbba92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758030ef-8e0e-4a6b-91c8-4bf28dfe57db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- AUTO-CLEANUP ---\n",
    "global results\n",
    "results = [r for r in results if not r['Test'].startswith(\"3.\")]\n",
    "\n",
    "# ==========================================\n",
    "# DIMENSION 3: DISK CACHE (Photon NVMe)\n",
    "# Scenario: Heavy Aggregation on the Liquid Table.\n",
    "# Cold: Reading from Object Storage (S3/ADLS).\n",
    "# Hot: Reading from Local SSD (No Network I/O).\n",
    "# ==========================================\n",
    "\n",
    "target_table = \"tbl_liquid\"\n",
    "\n",
    "# Define a heavy query (Aggregating 15M rows)\n",
    "def complex_aggregation():\n",
    "    spark.read.table(target_table) \\\n",
    "        .groupBy(\"event_date\") \\\n",
    "        .agg(F.sum(\"id\"), F.count(\"*\"), F.max(\"session_id\")) \\\n",
    "        .collect()\n",
    "\n",
    "# --- TEST A: COLD READ (Standard / No Cache) ---\n",
    "# We force clear the cache to simulate a fresh cluster or new data.\n",
    "spark.catalog.clearCache()\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")\n",
    "\n",
    "run_benchmark(\"3. Caching\", \"Cold Read (Standard)\", complex_aggregation)\n",
    "\n",
    "\n",
    "# --- WARMUP PHASE (Hidden) ---\n",
    "# We enable cache and run the query once to populate the SSDs.\n",
    "# We don't time this for the graph, but you can mention it takes the same time as Cold Read.\n",
    "print(\"ðŸ”¥ Warming up the cache...\")\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\n",
    "complex_aggregation() \n",
    "\n",
    "\n",
    "# --- TEST B: HOT READ (Optimized) ---\n",
    "# Now data is local. This simulates the experience of interactive users / BI Dashboards.\n",
    "run_benchmark(\"3. Caching\", \"Hot Read (Disk Cache)\", complex_aggregation)\n",
    "\n",
    "# --- SHOW COMPARISON ---\n",
    "display_results(\"3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259dbcf3-7691-4ebd-952e-09d11a2082a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d02a1b-664b-414c-a364-77adac3e55e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# --- AUTO-CLEANUP ---\n",
    "global results\n",
    "results = [r for r in results if not r['Test'].startswith(\"4.\")]\n",
    "\n",
    "# ==========================================\n",
    "# DIMENSION 4: ETL MERGE (Upserts)\n",
    "# We will increase the volume of updates to 1 Million \n",
    "# to overcome the startup overhead of the Low Shuffle algorithm.\n",
    "# ==========================================\n",
    "\n",
    "print(\"Generating 1M UPSERT rows (More volume to show the gap)...\")\n",
    "df_updates = spark.range(0, 1000000) \\\n",
    "    .withColumn(\"user_id\", (F.col(\"id\") % 500000).cast(IntegerType())) \\\n",
    "    .withColumn(\"session_id\", F.rand() * 1000000) \\\n",
    "    .withColumn(\"url\", F.lit(\"/updated/url/path\")) \\\n",
    "    .withColumn(\"event_date\", F.current_date()) \\\n",
    "    .withColumn(\"event_hour\", F.lit(12).cast(IntegerType()))\n",
    "\n",
    "df_updates.createOrReplaceTempView(\"source_updates\")\n",
    "\n",
    "# --- TEST A: STANDARD MERGE ---\n",
    "spark.conf.set(\"spark.databricks.delta.merge.enableLowShuffle\", \"false\")\n",
    "\n",
    "def merge_standard():\n",
    "    # Standard merge will struggle more with 1M rows of updates\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO tbl_liquid t USING source_updates s ON t.id = s.id\n",
    "        WHEN MATCHED THEN UPDATE SET t.url = s.url\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "run_benchmark(\"4. ETL Merge\", \"Standard Shuffle\", merge_standard)\n",
    "\n",
    "\n",
    "# --- TEST B: LOW SHUFFLE MERGE (Modern) ---\n",
    "spark.conf.set(\"spark.databricks.delta.merge.enableLowShuffle\", \"true\")\n",
    "\n",
    "def merge_optimized():\n",
    "    # Low Shuffle will excel at managing the 1M row merge efficiently\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO tbl_liquid t USING source_updates s ON t.id = s.id\n",
    "        WHEN MATCHED THEN UPDATE SET t.url = s.url\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "run_benchmark(\"4. ETL Merge\", \"Low Shuffle (Modern)\", merge_optimized)\n",
    "\n",
    "# --- SHOW COMPARISON ---\n",
    "display_results(\"4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1383ff6e-2b33-43d5-811f-2428cdff1438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91cc42d-0203-4533-863f-5297733f789e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- AUTO-CLEANUP ---\n",
    "global results\n",
    "results = [r for r in results if not r['Test'].startswith(\"5.\")]\n",
    "\n",
    "# ==========================================\n",
    "# DIMENSION 5: SEARCH INDEXING (Bloom Filters)\n",
    "# Strategy: \"The Haystack Problem\"\n",
    "# We create a highly fragmented table (1000 files).\n",
    "# Standard: Must open/close 1000 files (Slow Metadata/IO).\n",
    "# Bloom: Checks index, skips 999 files instantly.\n",
    "# ==========================================\n",
    "\n",
    "print(\"Creating a 'Fragmented' table (1000 files) to simulate a Data Lake...\")\n",
    "# Lemos a tabela liquid e a reescrevemos \"estilhaÃ§ada\" em 1000 pedaÃ§os\n",
    "# Isso forÃ§a o Spark a lidar com muitos arquivos (Metadata Overhead)\n",
    "df_frag = spark.read.table(\"tbl_liquid\")\n",
    "df_frag.repartition(1000).write.mode(\"overwrite\").saveAsTable(\"tbl_fragmented\")\n",
    "\n",
    "# Valor que nÃ£o existe\n",
    "missing_session_id = -999999\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# --- TEST A: STANDARD SCAN (No Index) ---\n",
    "def point_lookup_std():\n",
    "    # Vai ter que abrir 1000 arquivos para descobrir que o dado nÃ£o estÃ¡ lÃ¡.\n",
    "    count = spark.read.table(\"tbl_fragmented\").filter(f\"session_id = {missing_session_id}\").count()\n",
    "    print(f\"Standard scan finished. Found: {count}\")\n",
    "\n",
    "run_benchmark(\"5. Point Lookup\", \"Full Scan (Standard)\", point_lookup_std)\n",
    "\n",
    "\n",
    "# --- TEST B: BLOOM FILTER (Indexed) ---\n",
    "print(\"ðŸŒ¸ Creating Bloom Filter Index on 'tbl_fragmented'...\")\n",
    "spark.sql(\"CREATE BLOOMFILTER INDEX ON TABLE tbl_fragmented FOR COLUMNS(session_id)\")\n",
    "\n",
    "def point_lookup_bloom():\n",
    "    # O Spark vai olhar os Ã­ndices e pular 99.9% dos arquivos.\n",
    "    count = spark.read.table(\"tbl_fragmented\").filter(f\"session_id = {missing_session_id}\").count()\n",
    "    print(f\"Bloom Filter scan finished. Found: {count}\")\n",
    "\n",
    "run_benchmark(\"5. Point Lookup\", \"Bloom Filter (Modern)\", point_lookup_bloom)\n",
    "\n",
    "# --- SHOW COMPARISON ---\n",
    "display_results(\"5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04585a25-f8f8-4b22-95bc-2455936fd3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DevOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9857635-e816-4cd1-846b-b0debd6baea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- AUTO-CLEANUP ---\n",
    "global results\n",
    "results = [r for r in results if not r['Test'].startswith(\"6.\")]\n",
    "\n",
    "# ==========================================\n",
    "# DIMENSION 6: CLONING (DevOps / CI/CD)\n",
    "# Scenario: Creating a Staging environment from Production.\n",
    "# Problem: Small data is too fast to copy. We need HEAVY data to show the pain.\n",
    "# ==========================================\n",
    "\n",
    "print(\"Preparing HEAVY dataset for Cloning test...\")\n",
    "\n",
    "# 1. Create a Heavy Source Table\n",
    "# We take the existing rows and add a massive text column (payload).\n",
    "# This bloats the file size, forcing the Deep Clone to work hard copying bytes.\n",
    "df_heavy_clone = spark.read.table(\"tbl_liquid\") \\\n",
    "    .withColumn(\"payload\", F.expr(\"repeat('SIMULATING_PRODUCTION_DATA_VOLUME_FOR_BENCHMARK_', 50)\"))\n",
    "\n",
    "# Materialize the source table (Simulating a Production Table)\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_heavy_source\")\n",
    "df_heavy_clone.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"tbl_heavy_source\")\n",
    "\n",
    "print(\"Tables prepared. Starting Race...\")\n",
    "\n",
    "# --- TEST A: DEEP CLONE (Standard) ---\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_dev_physical\")\n",
    "\n",
    "def deep_clone():\n",
    "    # DISADVANTAGE: 'CLONE' (Deep) physically duplicates every byte of the heavy table.\n",
    "    # It consumes Storage ($$$) and Time (I/O).\n",
    "    spark.sql(\"CREATE TABLE tbl_dev_physical CLONE tbl_heavy_source\")\n",
    "\n",
    "run_benchmark(\"6. Cloning\", \"Deep Copy (Standard)\", deep_clone)\n",
    "\n",
    "\n",
    "# --- TEST B: SHALLOW CLONE (Modern) ---\n",
    "spark.sql(\"DROP TABLE IF EXISTS tbl_dev_virtual\")\n",
    "\n",
    "def shallow_clone():\n",
    "    # ADVANTAGE: 'SHALLOW CLONE' only copies the Metadata (Transaction Log).\n",
    "    # It points to the original files. Zero Data Copy. Instant.\n",
    "    spark.sql(\"CREATE TABLE tbl_dev_virtual SHALLOW CLONE tbl_heavy_source\")\n",
    "\n",
    "run_benchmark(\"6. Cloning\", \"Shallow Clone (Modern)\", shallow_clone)\n",
    "\n",
    "# --- SHOW COMPARISON ---\n",
    "display_results(\"6.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74c2eb3b-2526-4e6d-af8c-25cd30905932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eaecd41-46ab-454e-91b5-f98efc611b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# --- CONFIGURATION: CUSTOM TITLES ---\n",
    "# Separando Data Layout em WRITE e READ conforme solicitado\n",
    "custom_titles = {\n",
    "    \"1. Ingestion\":     \"Data Layout (Write): Ingestion Cost & Optimization Overhead\", \n",
    "    \"1. Read (Filter)\": \"Data Layout (Read): Standard vs. Z-Order vs. Liquid Clustering\",\n",
    "    \"2. Delete\":        \"DML Operations: Copy-on-Write vs. Deletion Vectors\",\n",
    "    \"3. Caching\":       \"Read Speed: Cold Read vs. Disk Cache (Hot)\",\n",
    "    \"4. ETL Merge\":     \"ETL Merge: Standard Shuffle vs. Low Shuffle Merge\",\n",
    "    \"5. Point Lookup\":  \"Search: Full Scan vs. Bloom Filters\",\n",
    "    \"6. Cloning\":       \"DevOps: Deep Clone vs. Shallow Clone\"\n",
    "}\n",
    "\n",
    "# 1. Prepare Data\n",
    "df_res = pd.DataFrame(results)\n",
    "\n",
    "# Clean up method names for chart readability\n",
    "df_res['Method_Short'] = df_res['Method']\n",
    "\n",
    "# Define Logic for \"Standard/Old\" vs \"New/Optimized\" colors\n",
    "# We treat 'Standard', 'Legacy', 'Cold', and 'Deep' as the baseline (Gray)\n",
    "df_res['Is_Baseline'] = df_res['Method'].str.contains(\"Standard\") | \\\n",
    "                        df_res['Method'].str.contains(\"Legacy\") | \\\n",
    "                        df_res['Method'].str.contains(\"Cold\") | \\\n",
    "                        df_res['Method'].str.contains(\"Deep\")\n",
    "\n",
    "# Get unique tests ensuring specific order\n",
    "ordered_tests = [\n",
    "    t for t in [\"1. Ingestion\", \"1. Read (Filter)\", \"2. Delete\", \"3. Caching\", \"4. ETL Merge\", \"5. Point Lookup\", \"6. Cloning\"] \n",
    "    if t in df_res['Test'].unique()\n",
    "]\n",
    "\n",
    "# 2. Setup Grid Layout (2 Columns)\n",
    "n_charts = len(ordered_tests)\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(n_charts / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Custom Colors: Gray for Old/Standard, Databricks Orange for New/Optimized\n",
    "palette = {True: \"#95a5a6\", False: \"#ff5722\"} # Gray vs Databricks Orange\n",
    "\n",
    "print(f\"--- ðŸ“Š DATABRICKS OPTIMIZATION BENCHMARK SUMMARY ---\")\n",
    "\n",
    "# 3. Generate Subplots\n",
    "for i, test_name in enumerate(ordered_tests):\n",
    "    ax = axes[i]\n",
    "    data = df_res[df_res['Test'] == test_name].copy()\n",
    "    \n",
    "    # Calculate Baseline for Speedup\n",
    "    # Logic: Find the row that represents the \"Old Way\" to set as 1.0x\n",
    "    try:\n",
    "        baseline = data[data['Is_Baseline'] == True]['Duration_Sec'].values[0]\n",
    "    except:\n",
    "        baseline = data['Duration_Sec'].max() # Fallback if specific baseline not found\n",
    "        \n",
    "    data['Speedup'] = baseline / data['Duration_Sec']\n",
    "    \n",
    "    # Plot Bar Chart\n",
    "    sns.barplot(\n",
    "        data=data, \n",
    "        y=\"Method\", \n",
    "        x=\"Duration_Sec\", \n",
    "        hue=\"Is_Baseline\", \n",
    "        ax=ax, \n",
    "        palette=palette, \n",
    "        dodge=False\n",
    "    )\n",
    "    \n",
    "    # Formatting\n",
    "    chart_title = custom_titles.get(test_name, test_name)\n",
    "    \n",
    "    ax.set_title(chart_title, fontsize=13, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel(\"Time (Seconds) - Lower is Better\", fontsize=10)\n",
    "    ax.set_ylabel(\"\")\n",
    "    \n",
    "    if ax.get_legend():\n",
    "        ax.get_legend().remove()\n",
    "        \n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add Text Annotations\n",
    "    for p, row_idx in zip(ax.patches, range(len(data))):\n",
    "        width = p.get_width()\n",
    "        if pd.isna(width): continue\n",
    "            \n",
    "        speedup = data.iloc[row_idx]['Speedup']\n",
    "        duration = data.iloc[row_idx]['Duration_Sec']\n",
    "        \n",
    "        # Smart Labeling\n",
    "        if speedup >= 1.1:\n",
    "            label = f\"ðŸš€ {speedup:.1f}x Faster\"\n",
    "            color = \"#d35400\" # Dark Orange\n",
    "            fontweight = 'bold'\n",
    "        elif speedup < 0.9:\n",
    "            # Highlight slowness (e.g., Z-Order Write Penalty)\n",
    "            label = f\"ðŸ”» {speedup:.1f}x (Slower)\"\n",
    "            color = \"#7f8c8d\" # Gray\n",
    "            fontweight = 'normal'\n",
    "        else:\n",
    "            label = \"1.0x (Baseline)\"\n",
    "            color = \"black\"\n",
    "            fontweight = 'normal'\n",
    "            \n",
    "        ax.text(\n",
    "            width + (baseline * 0.02), \n",
    "            p.get_y() + p.get_height() / 2, \n",
    "            f\"{duration:.2f}s  |  {label}\", \n",
    "            ha='left', va='center', fontsize=11, fontweight=fontweight, color=color\n",
    "        )\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle(\"Databricks Optimization Suite: Performance Benchmark\", fontsize=22, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Optimizations_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
