{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ab0018-9aa6-4136-8b93-2c13996ec7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![./ImageLab.png](./Images/ImageLab.png \"./ImageLab.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7107c90c-593d-401e-bc2f-3a4243520596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Engineering with Lakeflow, Jobs, AutoLoader and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efbb437-ddce-4af8-a335-03f356a98a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data for Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757f0d09-0f21-4af6-93de-335540c44c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read files from customers\n",
    "df_bronze=spark.read \\\n",
    "  .table(\"medallion.bronze.dim_customer\")\n",
    "\n",
    "df_bronze.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64e0613-c70b-4a92-ac34-31450793034c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Treating data\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Treating customer_bk\n",
    "df_bronze_treated = df_bronze \\\n",
    "  .withColumn(\"customer_bk\", F.col('customer_bk').cast(\"string\")) \\\n",
    "  .withColumn(\"customer_bk\", F.regexp_replace(F.col('customer_bk'), \"[^0-9]\", \"\"))\n",
    "\n",
    "# Treating customer_name\n",
    "df_bronze_treated = df_bronze_treated \\\n",
    "  .withColumn(\"customer_name\", F.trim(F.col('customer_name')))\n",
    "\n",
    "# Treating birth_date\n",
    "#Alternative 1 with coalesce\n",
    "# formats = [\"dd/MM/yyyy\", \"yyyy-MM-dd\", \"MM/dd/yyyy\", \"dd-MM-yyyy\"]\n",
    "# df_bronze_treated = df_bronze_treated.withColumn(\"birth_date\",F.coalesce(*[F.try_to_date(\"birth_date\", f) for f in formats]))\n",
    "\n",
    "# Alternative 2 using ai function\n",
    "df_bronze_treated = df_bronze_treated.selectExpr(\"* EXCEPT(birth_date)\",\"ai_query('databricks-meta-llama-3-3-70b-instruct',concat('Padronize as datas para o formato 2025-01-01. Apenas me dê o resulado final e nada mais: ', birth_date)) as birth_date\")\n",
    "\n",
    "# Treating segment\n",
    "df_bronze_treated = df_bronze_treated.withColumn(\"segment\", F.trim(F.initcap(F.col('segment'))))\n",
    "\n",
    "# Treating region\n",
    "df_bronze_treated = df_bronze_treated.selectExpr(\"* EXCEPT(region)\",\"ai_query('databricks-meta-llama-3-3-70b-instruct',concat('Padronize as regiões para o formato de sigla do estado do Brasil, como SP, MG, RJ, etc. Apenas me dê o resulado final e nada mais: ', region)) as region\")\n",
    "\n",
    "# Treating effective_ts\n",
    "df_bronze_treated = df_bronze_treated.withColumn(\"effective_ts\", F.date_format(F.trim(F.col('effective_ts')),\"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "\n",
    "df_bronze_treated.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfbf5d6-fb86-4ebd-a79b-e2c646d5aeae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating table using schema if it is first load\n",
    "from pyspark.sql.window import Window\n",
    "if not spark.catalog.tableExists(\"medallion.silver.dim_customer\"):\n",
    "    empty_df = df_bronze_treated\n",
    "\n",
    "    w_last = Window.partitionBy(\"customer_bk\").orderBy(F.col(\"effective_ts\").desc())\n",
    "    \n",
    "    empty_df = empty_df \\\n",
    "    .withColumn(\"rn\", F.row_number().over(w_last)) \\\n",
    "    .filter(F.col(\"rn\") == 1) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "    empty_df = empty_df \\\n",
    "        .withColumn(\"valid_from\", F.col(\"effective_ts\")) \\\n",
    "        .withColumn(\"valid_to\",   F.lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"version\", F.lit(1).cast(\"int\")) \\\n",
    "        .withColumn(\"customer_sk\", F.lit(None).cast(\"bigint\")) \\\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "    w_sk = Window.orderBy(\"customer_bk\", \"valid_from\", \"customer_name\", \"birth_date\")\n",
    "\n",
    "\n",
    "    empty_df = empty_df \\\n",
    "        .withColumn(\"customer_sk\", F.row_number().over(w_sk).cast(\"bigint\")) \\\n",
    "        .write.mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"medallion.silver.dim_customer\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Incremental executions\n",
    "else:\n",
    "    from delta.tables import DeltaTable\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "\n",
    "    w_last = Window.partitionBy(\"customer_bk\").orderBy(F.col(\"effective_ts\").desc())\n",
    "\n",
    "    df_bronze_dedup = (\n",
    "        df_bronze_treated\n",
    "        .select(\"customer_bk\", \"customer_name\", \"birth_date\", \"segment\", \"region\", \"effective_ts\", \"file_path\", \"file_mod_time\") \n",
    "    .withColumn(\"rn\", F.row_number().over(w_last))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "\n",
    "    # Using SCD type 2\n",
    "    delta = DeltaTable.forName(spark, \"medallion.silver.dim_customer\")\n",
    "    delta.alias(\"delta\").merge(source=df_bronze_dedup.alias(\"df\"), condition=\"\"\"\n",
    "                                                        df.customer_bk = delta.customer_bk\n",
    "                                                        \"\"\") \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"\"\"\n",
    "            delta.customer_name <> df.customer_name OR\n",
    "            delta.birth_date    <> df.birth_date    OR\n",
    "            delta.segment       <> df.segment       OR\n",
    "            delta.region        <> df.region\n",
    "        \"\"\",\n",
    "        set={\n",
    "            \"is_current\": F.lit(False),\n",
    "            \"valid_to\":   F.col(\"df.effective_ts\"),\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"customer_bk\":   F.col(\"df.customer_bk\"),\n",
    "            \"customer_name\": F.col(\"df.customer_name\"),\n",
    "            \"birth_date\":    F.col(\"df.birth_date\"),\n",
    "            \"segment\":       F.col(\"df.segment\"),\n",
    "            \"region\":        F.col(\"df.region\"),\n",
    "            \"valid_from\":    F.col(\"df.effective_ts\"),\n",
    "            \"valid_to\":      F.lit(None).cast(\"timestamp\"),\n",
    "            \"is_current\":    F.lit(True),\n",
    "            \"version\":       F.lit(1).cast(\"int\"),\n",
    "            \"customer_sk\":   F.lit(None).cast(\"bigint\"),\n",
    "            \"file_path\":     F.col(\"df.file_path\"),\n",
    "            \"file_mod_time\": F.col(\"df.file_mod_time\")\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()\n",
    "\n",
    "    dim_customer=spark.read.table(\"medallion.silver.dim_customer\")\n",
    "\n",
    "    # Adding version\n",
    "    version_null = dim_customer.filter(F.col(\"version\").isNull())\n",
    "\n",
    "    if version_null.count()>0:\n",
    "\n",
    "        existing_with_ver = dim_customer.filter(F.col(\"version\").isNotNull())\n",
    "\n",
    "        max_ver_por_bk = existing_with_ver.groupBy(\"customer_bk\") \\\n",
    "            .agg(F.max(\"version\").alias(\"max_version\"))\n",
    "\n",
    "        version_null = version_null.join(max_ver_por_bk, on=\"customer_bk\", how=\"left\") \\\n",
    "            .withColumn(\"max_version\", F.coalesce(F.col(\"max_version\"), F.lit(0)))\n",
    "\n",
    "\n",
    "\n",
    "        delta_version = DeltaTable.forName(spark, \"medallion.silver.dim_customer\")\n",
    "\n",
    "        delta_version.alias(\"delta_version\").merge(source=version_to_include.alias(\"version_to_include\"), condition=\"\"\"\n",
    "        delta_version.customer_bk = version_to_include.customer_bk AND\n",
    "        delta_version.valid_from  = version_to_include.valid_from AND\n",
    "        delta_version.valid_to    <=> version_to_include.valid_to AND\n",
    "        delta_version.is_current  = version_to_include.is_current\n",
    "                                                            \"\"\") \\\n",
    "        .whenMatchedUpdate(\n",
    "            set={\n",
    "                \"version\": F.col(\"version_to_include.version\")\n",
    "            }\n",
    "            ) \\\n",
    "        .execute()\n",
    "\n",
    "    dim_customer=spark.read.table(\"medallion.silver.dim_customer\")\n",
    "\n",
    "    # Adding sk\n",
    "    sk_null = dim_customer.filter(F.col(\"customer_sk\").isNull())\n",
    "\n",
    "    if sk_null.count()>0:\n",
    "        max_sk = dim_customer.agg(F.max(\"customer_sk\").alias(\"max_sk\")).collect()[0][\"max_sk\"]\n",
    "        if max_sk is None:\n",
    "            max_sk = 0\n",
    "\n",
    "        w = Window.orderBy(\"valid_from\", \"customer_bk\")\n",
    "\n",
    "        sk_to_include = sk_null.withColumn(\n",
    "            \"customer_sk\",\n",
    "            F.row_number().over(w) + F.lit(max_sk)\n",
    "        )\n",
    "\n",
    "        delta_sk = DeltaTable.forName(spark, \"medallion.silver.dim_customer\")\n",
    "\n",
    "        delta_sk.alias(\"delta_sk\").merge(source=sk_to_include.alias(\"sk_to_include\"), condition=\"\"\"\n",
    "        delta_sk.customer_bk = sk_to_include.customer_bk AND\n",
    "        delta_sk.valid_from  = sk_to_include.valid_from AND\n",
    "        delta_sk.valid_to    <=> sk_to_include.valid_to AND\n",
    "        delta_sk.is_current  = sk_to_include.is_current\n",
    "                                                            \"\"\") \\\n",
    "        .whenMatchedUpdate(\n",
    "            set={\n",
    "                \"customer_sk\": F.col(\"sk_to_include.customer_sk\")\n",
    "            }\n",
    "            ) \\\n",
    "        .execute()\n",
    "\n",
    "\n",
    "    dim_customer = spark.read.table(\"medallion.silver.dim_customer\")\n",
    "    dim_current  = dim_customer.filter(F.col(\"is_current\") == True)\n",
    "\n",
    "    # Incremental join versus actual dimension\n",
    "    df_join = df_bronze_dedup.alias(\"df\").join(\n",
    "        dim_current.alias(\"delta\"),\n",
    "        on=[F.col(\"df.customer_bk\") == F.col(\"delta.customer_bk\")],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # New BK (don't exist)\n",
    "    df_new = df_join.filter(F.col(\"delta.customer_bk\").isNull()) \\\n",
    "                    .select(\"df.*\")\n",
    "\n",
    "    # Existant BK with changes(already closed by whenMatchedUpdate)\n",
    "    df_changed = df_join.filter(\n",
    "        (F.col(\"delta.customer_bk\").isNotNull()) &\n",
    "        (\n",
    "            (F.col(\"delta.customer_name\") != F.col(\"df.customer_name\")) |\n",
    "            (F.col(\"delta.birth_date\")    != F.col(\"df.birth_date\"))    |\n",
    "            (F.col(\"delta.segment\")       != F.col(\"df.segment\"))       |\n",
    "            (F.col(\"delta.region\")        != F.col(\"df.region\"))\n",
    "        )\n",
    "    ).select(\"df.*\")\n",
    "\n",
    "    # Remaining rows that will be inserted in the table (new + new versions)\n",
    "    df_remaining = df_new.unionByName(df_changed)\n",
    "\n",
    "    # Adiciona colunas SCD: valid_from, valid_to, is_current\n",
    "    df_remaining = (\n",
    "        df_remaining\n",
    "        .withColumn(\"valid_from\", F.col(\"effective_ts\"))\n",
    "        .withColumn(\"valid_to\",   F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "    )\n",
    "\n",
    "    # VERSION: next version by customer_bk\n",
    "    max_ver_por_bk = dim_customer.groupBy(\"customer_bk\") \\\n",
    "        .agg(F.max(\"version\").alias(\"max_version\"))\n",
    "\n",
    "    df_remaining= df_remaining.join(\n",
    "        max_ver_por_bk,\n",
    "        on=\"customer_bk\",\n",
    "        how=\"left\"\n",
    "    ).withColumn(\n",
    "        \"max_version\",\n",
    "        F.coalesce(F.col(\"max_version\"), F.lit(0))\n",
    "    ).withColumn(\n",
    "        \"version\",\n",
    "        (F.col(\"max_version\") + F.lit(1)).cast(\"int\")\n",
    "    ).drop(\"max_version\")\n",
    "\n",
    "    # CUSTOMER_SK: generate SK only for these rows\n",
    "    max_sk = dim_customer.agg(F.max(\"customer_sk\").alias(\"max_sk\")).collect()[0][\"max_sk\"]\n",
    "    if max_sk is None:\n",
    "        max_sk = 0\n",
    "\n",
    "    w_sk = Window.orderBy(\"valid_from\", \"customer_bk\", \"customer_name\", \"birth_date\")\n",
    "\n",
    "    df_remaining = df_remaining.withColumn(\n",
    "        \"customer_sk\",\n",
    "        (F.row_number().over(w_sk) + F.lit(max_sk)).cast(\"bigint\")\n",
    "    )\n",
    "\n",
    "    # Now insert only the remaining rows\n",
    "    delta = DeltaTable.forName(spark, \"medallion.silver.dim_customer\")\n",
    "\n",
    "    (\n",
    "        delta.alias(\"delta\")\n",
    "        .merge(\n",
    "            source=df_remaining.alias(\"df\"),\n",
    "            condition=\"delta.customer_sk = df.customer_sk\"\n",
    "        )\n",
    "        .whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"customer_sk\":   F.col(\"df.customer_sk\"),\n",
    "                \"customer_bk\":   F.col(\"df.customer_bk\"),\n",
    "                \"customer_name\": F.col(\"df.customer_name\"),\n",
    "                \"birth_date\":    F.col(\"df.birth_date\"),\n",
    "                \"segment\":       F.col(\"df.segment\"),\n",
    "                \"region\":        F.col(\"df.region\"),\n",
    "                \"valid_from\":    F.col(\"df.valid_from\"),\n",
    "                \"valid_to\":      F.col(\"df.valid_to\"),   # always NULL here\n",
    "                \"is_current\":    F.col(\"df.is_current\"), # always TRUE here\n",
    "                \"version\":       F.col(\"df.version\"),\n",
    "                \"file_path\":     F.col(\"df.file_path\"),\n",
    "                \"file_mod_time\": F.col(\"df.file_mod_time\")\n",
    "            }\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "# Verify data\n",
    "display(spark.read.table(\"medallion.silver.dim_customer\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_DataEngineering_Batch_Incremental_NoAutoloader_SCDType2_ManualCDC_CleanData_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
